{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have the preprocessing done, let us focus on the _train.py_ file with all the code required to finetune an LLM on our specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us define the **MedData** class where we are going to save our specific med data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/med-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries for Data Utils\n",
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from huggingface_hub import login\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase, BitsAndBytesConfig)\n",
    "from transformers.utils import PaddingStrategy\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "# Importing libraries for Model Utils\n",
    "import lightning as L\n",
    "# The Accelerator is the main class for enabling distributed training on any type of training setup\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from callback_utils import GenerateText # TODO: to define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the DataCollatorWithPaddingAndLabel class.\n",
    "In this class we are using the __@dataclass__ decorator.\n",
    "As written here (https://dzone.com/articles/understanding-pythons-dataclass-decorator), _In a nutshell, the primary goal of the @dataclass decorator is to simplify the creation of classes._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorWithPaddingAndLabels:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding : Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, samples: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "        # convert into a dict with lists\n",
    "        features_list = {key: [] for key in samples[0].keys()}\n",
    "        for sample in samples:\n",
    "            for key, val in sample.items():\n",
    "                features_list[key].append(val)\n",
    "        \n",
    "        batch = {} # We are preparing the structure we are going to return back\n",
    "        for key, val in features_list.item():\n",
    "            if \"input_ids\" in key:\n",
    "                padded = self.tokenizer.pad(\n",
    "                    {'input_ids':val},\n",
    "                    padding=self.padding,\n",
    "                    max_length=self.max_length,\n",
    "                    pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                    return_tensors=self.return_tensors,\n",
    "                    return_attention_mask=True\n",
    "                )\n",
    "                batch[key] = padded['input_ids']\n",
    "                batch[key.replace('input_ids', 'attention_mask')] = padded['attention_mask']\n",
    "            elif \"labels\" in key: # not used here\n",
    "                batch[key] = self.tokenizer.pad(\n",
    "                    {'input_ids':val},\n",
    "                    padding=self.padding,\n",
    "                    max_length=self.max_length,\n",
    "                    pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                    return_tensors=self.return_tensors,\n",
    "                    return_attention_mask=True\n",
    "                )['input_ids']\n",
    "            else:\n",
    "                batch[key] = val\n",
    "        return batch     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us introduce the tokenize_sample function\n",
    "def tokenize_sample(tokenizer, sample, features, add_special_tokens=False, eos_token=False, postpend=\"\"):\n",
    "    input_text = [f\"\\n[{key}]\\n{sample[key]}\" for key in features]\n",
    "    input_text = \" \".join(input_text)\n",
    "    input_tokens = tokenizer(input_text, return_attention_mask=False, add_special_tokens=add_special_tokens)\n",
    "    if eos_token:\n",
    "        input_tokens['input_ids'] += [tokenizer.eos_token_id]\n",
    "    \n",
    "    if postpend != \"\":\n",
    "        postpend_tokens = tokenizer.encode(postpend, return_tensors=\"pt\", add_special_tokens=False).squeeze(0).tolist()\n",
    "        input_tokens['input_ids'] += postpend_tokens\n",
    "    \n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedData(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds_hf, tokenizer):\n",
    "        self.ds_hf = ds_hf\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = 2048\n",
    "\n",
    "        # Create prompt features\n",
    "        self.all_features = ['static', 'event', 'death_status'] # same as self.ds_hf.features.keys()\n",
    "        self.prompt_features = self.all_features[:-1]\n",
    "        self.outcome_feature = self.all_features[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds_hf)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ds_hf[idx]\n",
    "        full_text = tokenize_sample(tokenizer=self.tokenizer, sample=sample, features=self.all_features, eos_token=True)\n",
    "        if len(full_text['input_ids']) > self.max_length:\n",
    "            # we need to cut but we cut by some more to allow for eos etc\n",
    "            cut_by = 10+len(full_text['input_ids']) - self.max_length\n",
    "            encoded_and_cut = self.tokenizer(sample['event'])['input_ids'][cut_by:] # let us cut from left (truncating older events)\n",
    "            sample['event'] = \"(...)\" + self.tokenizer.decode(encoded_and_cut)\n",
    "            full_text = tokenize_sample(tokenizer=self.tokenizer, sample=sample, features=self.all_features, eos_token=True)\n",
    "        \n",
    "        for key, val in full_text.items():\n",
    "            sample[key] = val\n",
    "        \n",
    "        prompt = tokenize_sample(tokenizer=self.tokenizer, sample=sample, features=self.prompt_features, postpend=f\"[{self.outcome_feature}]\\n\")\n",
    "\n",
    "        for key, val in prompt.items():\n",
    "            sample[f\"prompt.{key}\"]=val\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloaders(tokenizer, params):\n",
    "    data = datasets.load_from_disk(\"\")\n",
    "    ds = {phase : MedData(ds_hf, tokenizer=tokenizer) for phase, ds_hf in data.items()}\n",
    "    data_collate = DataCollatorWithPaddingAndLabels(tokenizer=tokenizer, max_length=2048)\n",
    "    dl = {\n",
    "        phase: torch.utils.data.DataLoader(\n",
    "            ds, \n",
    "            batch_size=1, \n",
    "            shuffle=True if phase==\"train\" else False, \n",
    "            collate_fn=data_collate) \n",
    "        for phase, ds in ds.items()\n",
    "        }\n",
    "    return dl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once we had all the data utils settled down we can focus more on the model utils (we need a GPU to run them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLightning(L.LightningModule):\n",
    "    def __init__(self, model, tokenizer, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model', 'tokenizer'])\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None, *args, **kwargs):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "    def step(self, batch, phase):\n",
    "        output = self.forward(batch['input_ids'], batch['attention_mask'], batch['input_ids']) # TODO: to understand better\n",
    "        loss = output.loss\n",
    "        self.log(f\"{phase}/loss\", loss) # TODO: how does it work log?\n",
    "        return loss\n",
    "    \n",
    "    def traininig_step(self, batch, batch_idx):\n",
    "        return self.step(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, 'validation')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, 'test')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating a function __load_model()__ which is responsible for taking the model we want to train and load it trough the __prepare_model_for_kbit_training__ method, which wraps the entire protocol for preparing a model before running a training.\n",
    "Here there are some hyperparameters, both for the model itself (__AutoModelForCausalLM__) but also for LoRA adapter (__LoraConfig__).\n",
    "\n",
    "Some references:\n",
    "    - LoRA Parameters in general: https://huggingface.co/docs/peft/package_reference/lora\n",
    "    - PEFT parameters (e.g. inference_mode): https://huggingface.co/docs/peft/quicktour\n",
    "    - LoRA alpha : https://datascience.stackexchange.com/questions/123229/understanding-alpha-parameter-tuning-in-lora-paper\n",
    "\n",
    "It still somehow unclear how tune these hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should have params in the name\n",
    "def load_model(params):\n",
    "    device_index = Accelerator().process_index\n",
    "    device_map = {\"\": device_index}\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "       params[\"model_name\"],\n",
    "        trust_remote_code=True, \n",
    "        load_in_8bit=params.get(\"load_in_8bit\", False),\n",
    "        load_in_4bit=params.get(\"load_in_4bit\", False),\n",
    "        device_map=device_map\n",
    "    )\n",
    "\n",
    "    # build adapters per task\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=params[\"lora_dim\"]\n",
    "        lora_alpha=params[\"lora_alpha\"]\n",
    "        lora_dropout=params[\"lora_dropout\"],\n",
    "        target_modules=params[\"lora_target_modules\"]\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Add adapters\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(model.print_trainable_parameters())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pl_module(tokenizer, params, checkpoint_path=None):\n",
    "    print(\"build_model...\")\n",
    "    model = load_model(params)\n",
    "    if checkpoint_path:\n",
    "        print(\"loading from checkpoint...\")\n",
    "        pl_module = ModelLightning.load_from_checkpoint(checkpoint_path, model=model, tokenizer=tokenizer, learning_rate=params['learning_rate'])\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        pl_module = ModelLightning(model=model, tokenizer=tokenizer, learning_rate=params['learning_rate'])\n",
    "    return pl_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define the __params__ dictionary containing all the parameters useful for generalizing all this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "params = {\n",
    "    \"model_name\": \"meta-llama/Llama-3.1-8B\",\n",
    "    'accumulate_grad_batches': 16,\n",
    "    'precision': 16,\n",
    "    'val_check_interval': 0.25,\n",
    "    'max_epochs': 100,\n",
    "    'batch_size': 1,\n",
    "    'max_length': 2048,\n",
    "    'learning_rate': 1e-6,\n",
    "    # lora parameters\n",
    "    'load_in_8bit' : True,\n",
    "    \"lora_dim\": 256,\n",
    "    \"lora_alpha\": 256,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    'lora_target_modules': None\n",
    "}\n",
    "params['name'] = params['model_name'].split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    login(\"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\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"build dataloaders...\")\n",
    "    dl = build_dataloaders(tokenizer, params)\n",
    "    checkpoint_path = None\n",
    "    pl_module = load_pl_module(tokenizer, params, checkpoint_path=checkpoint_path)\n",
    "\n",
    "    # Let us create a tensorboard logger: # TODO: to understand\n",
    "    callbacks = [\n",
    "        GenerateText(dataloaders=dl, max_token_len=params['max_length']),\n",
    "        L.pytorch.callbacks.ModelCheckpoint(monitor=\"validation/loss\")]\n",
    "    print(\"start training..\")\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=params['name'])\n",
    "    trainer = L.Trainer(\n",
    "        logger=logger,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=params.get('max_epochs', 10),\n",
    "        accumulate_grad_batches=params.get('accumulate_grad_batches', 1),\n",
    "        precision=params.get('precision', '16-mixed'),\n",
    "        val_check_interval=params.get('val_check_interval', 0.5),\n",
    "        )\n",
    "\n",
    "    trainer.fit(model=pl_module, train_dataloaders=dl['train'],val_dataloaders= dl['validation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
