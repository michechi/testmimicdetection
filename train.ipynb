{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have the preprocessing done, let us focus on the _train.py_ file with all the code required to finetune an LLM on our specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us define the **MedData** class where we are going to save our specific med data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/huggingface/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries for Data Utils\n",
    "import torch\n",
    "import datasets\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from huggingface_hub import login\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase, BitsAndBytesConfig)\n",
    "from transformers.utils import PaddingStrategy\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "\n",
    "# Importing libraries for Model Utils\n",
    "import lightning as L\n",
    "import bitsandbytes\n",
    "# The Accelerator is the main class for enabling distributed training on any type of training setup\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, get_peft_model\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from callback_utils_mimic import GenerateText # TODO: to define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the DataCollatorWithPaddingAndLabel class.\n",
    "In this class we are using the __@dataclass__ decorator.\n",
    "As written here (https://dzone.com/articles/understanding-pythons-dataclass-decorator), _In a nutshell, the primary goal of the @dataclass decorator is to simplify the creation of classes._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorWithPaddingAndLabels:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding : Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, samples: Dict[str, List[Any]]) -> Dict[str, Any]:\n",
    "        # convert into a dict with lists\n",
    "        features_list = {key: [] for key in samples[0].keys()}\n",
    "        for sample in samples:\n",
    "            for key, val in sample.items():\n",
    "                features_list[key].append(val)\n",
    "        \n",
    "        batch = {} # We are preparing the structure we are going to return back\n",
    "        for key, val in features_list.items():\n",
    "            if \"input_ids\" in key:\n",
    "                padded = self.tokenizer.pad(\n",
    "                    {'input_ids':val},\n",
    "                    padding=self.padding,\n",
    "                    max_length=self.max_length,\n",
    "                    pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                    return_tensors=self.return_tensors,\n",
    "                    return_attention_mask=True\n",
    "                )\n",
    "                batch[key] = torch.Tensor(padded['input_ids']).to('cuda:0')\n",
    "                batch[key.replace('input_ids', 'attention_mask')] = padded['attention_mask'].to('cuda:0')\n",
    "            elif \"labels\" in key: # not used here\n",
    "                batch[key] = self.tokenizer.pad(\n",
    "                    {'input_ids':val},\n",
    "                    padding=self.padding,\n",
    "                    max_length=self.max_length,\n",
    "                    pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                    return_tensors=self.return_tensors,\n",
    "                    return_attention_mask=True\n",
    "                )['input_ids'].to('cuda:0')\n",
    "            else:\n",
    "                batch[key] = val\n",
    "            \n",
    "        return batch     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.Tensor(1).to(\"cuda:0\").device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us introduce the tokenize_sample function\n",
    "def tokenize_sample(tokenizer, sample, features, add_special_tokens=False, eos_token=False, postpend=\"\"):\n",
    "    input_text = [f\"\\n[{key}]\\n{sample[key]}\" for key in features]\n",
    "    input_text = \" \".join(input_text)\n",
    "    input_tokens = tokenizer(input_text, return_attention_mask=False, add_special_tokens=add_special_tokens)\n",
    "    if eos_token:\n",
    "        input_tokens['input_ids'] += [tokenizer.eos_token_id]\n",
    "    \n",
    "    if postpend != \"\":\n",
    "        postpend_tokens = tokenizer.encode(postpend, return_tensors=\"pt\", add_special_tokens=False).squeeze(0).tolist()\n",
    "        input_tokens['input_ids'] += postpend_tokens\n",
    "    return input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedData(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds_hf, tokenizer):\n",
    "        self.ds_hf = ds_hf\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = 2048\n",
    "\n",
    "        # Create prompt features\n",
    "        self.all_features = ['static', 'event', 'death_status'] # same as self.ds_hf.features.keys()\n",
    "        self.prompt_features = self.all_features[:-1]\n",
    "        self.outcome_feature = self.all_features[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds_hf)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.ds_hf[idx]\n",
    "        full_text = tokenize_sample(tokenizer=self.tokenizer, sample=sample, features=self.all_features, eos_token=True)\n",
    "        if len(full_text['input_ids']) > self.max_length:\n",
    "            # we need to cut but we cut by some more to allow for eos etc\n",
    "            cut_by = 10+len(full_text['input_ids']) - self.max_length\n",
    "            encoded_and_cut = self.tokenizer(sample['event'])['input_ids'][cut_by:] # let us cut from left (truncating older events)\n",
    "            sample['event'] = \"(...)\" + self.tokenizer.decode(encoded_and_cut)\n",
    "            full_text = tokenize_sample(tokenizer=self.tokenizer, sample=sample, features=self.all_features, eos_token=True)\n",
    "        \n",
    "        for key, val in full_text.items():\n",
    "            sample[key] = val\n",
    "        \n",
    "        prompt = tokenize_sample(tokenizer=self.tokenizer, sample=sample, features=self.prompt_features, postpend=f\"[{self.outcome_feature}]\\n\")\n",
    "\n",
    "        for key, val in prompt.items():\n",
    "            sample[f\"prompt.{key}\"]=val\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloaders(tokenizer, params):\n",
    "    data = datasets.load_from_disk(\"\")\n",
    "    ds = {phase : MedData(ds_hf, tokenizer=tokenizer) for phase, ds_hf in data.items()}\n",
    "    data_collate = DataCollatorWithPaddingAndLabels(tokenizer=tokenizer, max_length=params['max_length'])\n",
    "    dl = {\n",
    "        phase: torch.utils.data.DataLoader(\n",
    "            ds, \n",
    "            batch_size=params['batch_size'], \n",
    "            shuffle=True if phase==\"train\" else False, \n",
    "            collate_fn=data_collate) \n",
    "        for phase, ds in ds.items()\n",
    "        }\n",
    "    return dl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once we had all the data utils settled down we can focus more on the model utils (we need a GPU to run them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLightning(L.LightningModule):\n",
    "    def __init__(self, model, tokenizer, learning_rate):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['model', 'tokenizer'])\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels=None, *args, **kwargs):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "    def step(self, batch, phase):\n",
    "        output = self.forward(batch['input_ids'], batch['attention_mask'], batch['input_ids']) # TODO: to understand better\n",
    "        loss = output.loss\n",
    "        self.log(f\"{phase}/loss\", loss) # TODO: how does it work log?\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, 'validation')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, 'test')\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating a function __load_model()__ which is responsible for taking the model we want to train and load it trough the __prepare_model_for_kbit_training__ method, which wraps the entire protocol for preparing a model before running a training.\n",
    "Here there are some hyperparameters, both for the model itself (__AutoModelForCausalLM__) but also for LoRA adapter (__LoraConfig__).\n",
    "\n",
    "Some references:\n",
    "    - LoRA Parameters in general: https://huggingface.co/docs/peft/package_reference/lora\n",
    "    - PEFT parameters (e.g. inference_mode): https://huggingface.co/docs/peft/quicktour\n",
    "    - LoRA alpha : https://datascience.stackexchange.com/questions/123229/understanding-alpha-parameter-tuning-in-lora-paper\n",
    "\n",
    "It still somehow unclear how tune these hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should have params in the name\n",
    "def load_model(params):\n",
    "    # device_index = Accelerator().process_index\n",
    "    # device_map = {'': device(type='cuda', index=0)}\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        params[\"model_name\"],\n",
    "        trust_remote_code=True, \n",
    "        quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n",
    "        device_map=\"cuda:0\"\n",
    "    )\n",
    "\n",
    "    # build adapters per task\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=params[\"lora_dim\"],\n",
    "        lora_alpha=params[\"lora_alpha\"],\n",
    "        lora_dropout=params[\"lora_dropout\"],\n",
    "        target_modules=params[\"lora_target_modules\"]\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Add adapters\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(model.print_trainable_parameters())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pl_module(tokenizer, params, checkpoint_path=None):\n",
    "    print(\"build_model...\")\n",
    "    model = load_model(params)\n",
    "    if checkpoint_path:\n",
    "        print(\"loading from checkpoint...\")\n",
    "        pl_module = ModelLightning.load_from_checkpoint(checkpoint_path, model=model, tokenizer=tokenizer, learning_rate=params['learning_rate'])\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        pl_module = ModelLightning(model=model, tokenizer=tokenizer, learning_rate=params['learning_rate'])\n",
    "    return pl_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define the __params__ dictionary containing all the parameters useful for generalizing all this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "# I need also to investigate better how to tune all these parameters\n",
    "params = {\n",
    "    \"model_name\": \"meta-llama/Llama-3.1-8B\",\n",
    "    'accumulate_grad_batches': 16,\n",
    "    'precision': 16,\n",
    "    'val_check_interval': 0.25,\n",
    "    'max_epochs': 50,\n",
    "    'batch_size': 16,\n",
    "    'max_length': 2048,\n",
    "    'learning_rate': 1e-6,\n",
    "    # lora parameters\n",
    "    'load_in_8bit' : True,\n",
    "    \"lora_dim\": 256,\n",
    "    \"lora_alpha\": 256,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    'lora_target_modules': None\n",
    "}\n",
    "params['name'] = params['model_name'].split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    login(\"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\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"build dataloaders...\")\n",
    "    dl = build_dataloaders(tokenizer, params)\n",
    "    checkpoint_path = None\n",
    "    pl_module = load_pl_module(tokenizer, params, checkpoint_path=checkpoint_path)\n",
    "\n",
    "    # Let us create a tensorboard logger: # TODO: to understand\n",
    "    callbacks = [\n",
    "        GenerateText(dataloaders=dl, max_token_len=params['max_length']),\n",
    "        L.pytorch.callbacks.ModelCheckpoint(monitor=\"validation/loss\")]\n",
    "    print(\"start training..\")\n",
    "    logger = L.pytorch.loggers.TensorBoardLogger(\"tb_logs\", name=params['name'])\n",
    "    trainer = L.Trainer(\n",
    "        logger=logger,\n",
    "        callbacks=callbacks,\n",
    "        max_epochs=params.get('max_epochs', 10),\n",
    "        accumulate_grad_batches=params.get('accumulate_grad_batches', 1),\n",
    "        precision=params.get('precision', '16-mixed'),\n",
    "        val_check_interval=params.get('val_check_interval', 0.5),\n",
    "        devices=1\n",
    "        )\n",
    "\n",
    "    trainer.fit(model=pl_module, train_dataloaders=dl['train'],val_dataloaders= dl['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of launching all that code in one time let us see each component singularly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build dataloaders...\n"
     ]
    }
   ],
   "source": [
    "login(\"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\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"build dataloaders...\")\n",
    "dl = build_dataloaders(tokenizer, params)\n",
    "checkpoint_path = \"tb_logs/Llama-3.1-8B//version_10/checkpoints/epoch=2-step=275.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 109,051,904 || all params: 8,139,313,152 || trainable%: 1.3398\n",
      "None\n",
      "loading from checkpoint...\n"
     ]
    }
   ],
   "source": [
    "pl_module = load_pl_module(tokenizer, params, checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/huggingface/lib/python3.13/site-packages/lightning/fabric/connector.py:572: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training..\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "        GenerateText(dataloaders=dl, max_token_len=params['max_length']),\n",
    "        L.pytorch.callbacks.ModelCheckpoint(monitor=\"validation/loss\")]\n",
    "print(\"start training..\")\n",
    "logger = L.pytorch.loggers.TensorBoardLogger(\"tb_logs\", name=params['name'])\n",
    "trainer = L.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    "    max_epochs=params.get('max_epochs', 10),\n",
    "    accumulate_grad_batches=params.get('accumulate_grad_batches', 1),\n",
    "    precision=params.get('precision', '16-mixed'),\n",
    "    val_check_interval=params.get('val_check_interval', 0.5),\n",
    "    devices=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 8.1 B  | train\n",
      "-------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "8.0 B     Non-trainable params\n",
      "8.1 B     Total params\n",
      "32,557.253Total estimated model params size (MB)\n",
      "642       Modules in train mode\n",
      "423       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/huggingface/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=21` in the `DataLoader` to improve performance.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/huggingface/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2699: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/root/miniforge3/envs/huggingface/lib/python3.13/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/envs/huggingface/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/miniforge3/envs/huggingface/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=pl_module, train_dataloaders=dl['train'],val_dataloaders= dl['validation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
